

# 逻辑回归

逻辑回归一句话概括：逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的，是一种常见的分类算法。

1、逻辑回归的基本假设任何模型都是有自己的假设，在这个假设下模型才适用。逻辑回归的基本假设是数据服从伯努利分布。


2、逻辑回归的损失函数逻辑回归的损失函数是它的极大似然函数（交叉熵，等同于对数损失）


3、逻辑回归的求解方法由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，这三种方式的优劣以及如何选择最合适的梯度下降方式。


批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。

随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，迭代速度快，缺点是使得收敛到局部最优解的过程更加的复杂，得到的可能是局部最优解。

小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。如何选择合适的学习率？刚开始训练时，学习率可以稍微大一些，尽快逼近最优解，当学习到后面时，如果还保持原来的学习率，可能会越过最优点，在最优点附近振荡，此时，可以减小学习率。


4、逻辑回归的目的将数据二分类，提高准确率


5、逻辑回归如何分类逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值（逻辑回归直接输出概率值）确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。二分类、一般会选择0.5做为阈值来划分。


6、逻辑回归的损失函数为什么使用极大似然函数作为损失函数？为什么不选平方损失函数？在逻辑回归这个模型下，极大似然损失函数的训练求解参数快，和sigmod函数本身的梯度无关；如果使用平方损失函数，会发现梯度更新的速度和sigmod函数本身的梯度相关，这样训练会非常慢。


7、逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。


8、为什么我们还是会在训练的过程当中将高度相关的特征去掉？去掉高度相关的特征会让模型的可解释性更好可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。


9、逻辑回归的优缺点优点：模型形式简单、模型可解释性好、实现简单、占用资源少、效果也不错缺点：准确率并不高、很难处理数据不平衡的问题，如果不加入其它方法的情况下很难处理非线性问题
